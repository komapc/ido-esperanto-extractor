#!/usr/bin/env python3
"""
Merge morphological rules with extracted word entries.

This script combines the preserved morphological rules (paradigms) with
the word entries generated by the extractor, creating a complete dictionary.
"""

import argparse
import sys
from pathlib import Path
from xml.etree import ElementTree as ET
from typing import List, Dict, Any
import json
import logging

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def load_rules_file(rules_file: Path) -> ET.Element:
    """Load and parse the morphological rules file."""
    try:
        tree = ET.parse(rules_file)
        return tree.getroot()
    except ET.ParseError as e:
        logger.error(f"Failed to parse rules file {rules_file}: {e}")
        sys.exit(1)
    except FileNotFoundError:
        logger.error(f"Rules file not found: {rules_file}")
        sys.exit(1)

def load_extractor_words(extractor_file: Path) -> List[Dict[str, Any]]:
    """Load word entries from extractor output."""
    try:
        with open(extractor_file, 'r', encoding='utf-8') as f:
            if extractor_file.suffix == '.json':
                return json.load(f)
            else:
                # Handle other formats if needed
                logger.error(f"Unsupported extractor file format: {extractor_file.suffix}")
                sys.exit(1)
    except FileNotFoundError:
        logger.error(f"Extractor file not found: {extractor_file}")
        sys.exit(1)
    except json.JSONDecodeError as e:
        logger.error(f"Failed to parse extractor JSON: {e}")
        sys.exit(1)

def create_word_entry(word_data: Dict[str, Any]) -> ET.Element:
    """Create a dictionary entry element from word data."""
    entry = ET.Element('e')
    
    # Create lemma element
    lemma = ET.SubElement(entry, 'i')
    lemma.text = word_data.get('lemma', '')
    
    # Create paradigm reference
    par = ET.SubElement(entry, 'par')
    par.set('n', word_data.get('paradigm', 'default'))
    
    return entry

def merge_dictionary(rules_root: ET.Element, words_data: List[Dict[str, Any]], output_file: Path) -> None:
    """Merge rules with word entries and write to output file."""
    # Create a copy of the rules root
    merged_root = ET.fromstring(ET.tostring(rules_root, encoding='unicode'))
    
    # Find or create the main section
    main_section = merged_root.find('.//section[@id="main"]')
    if main_section is None:
        # Create main section if it doesn't exist
        main_section = ET.SubElement(merged_root, 'section')
        main_section.set('id', 'main')
        main_section.set('type', 'standard')
    
    # Clear existing entries (keep only the regex number entry)
    existing_entries = main_section.findall('e')
    for entry in existing_entries:
        # Keep the number regex entry
        par = entry.find('par')
        if par is not None and par.get('n') == 'num_regex':
            continue
        main_section.remove(entry)
    
    # Add word entries
    for word_data in words_data:
        entry = create_word_entry(word_data)
        main_section.append(entry)
    
    # Write the merged dictionary
    try:
        # Format the XML nicely
        ET.indent(merged_root, space="  ", level=0)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write('<?xml version="1.0" encoding="UTF-8"?>\n')
            f.write(ET.tostring(merged_root, encoding='unicode'))
        
        logger.info(f"Successfully merged dictionary: {output_file}")
        logger.info(f"Added {len(words_data)} word entries")
        
    except Exception as e:
        logger.error(f"Failed to write merged dictionary: {e}")
        sys.exit(1)

def validate_dictionary(dictionary_file: Path) -> bool:
    """Validate the generated dictionary file."""
    try:
        ET.parse(dictionary_file)
        logger.info("Dictionary validation: PASSED")
        return True
    except ET.ParseError as e:
        logger.error(f"Dictionary validation: FAILED - {e}")
        return False

def main():
    parser = argparse.ArgumentParser(description='Merge morphological rules with extracted words')
    parser.add_argument('--rules-file', type=Path, 
                       default=Path('rules/apertium-ido.ido.dix.rules'),
                       help='Path to morphological rules file')
    parser.add_argument('--extractor-file', type=Path,
                       help='Path to extractor output file (JSON format)')
    parser.add_argument('--output-file', type=Path,
                       default=Path('../../apertium/apertium-ido-epo/apertium-ido.ido.dix'),
                       help='Output dictionary file path')
    parser.add_argument('--test-mode', action='store_true',
                       help='Test mode: use sample data instead of extractor file')
    parser.add_argument('--validate', action='store_true',
                       help='Validate the output dictionary')
    
    args = parser.parse_args()
    
    # Load rules
    logger.info(f"Loading rules from: {args.rules_file}")
    rules_root = load_rules_file(args.rules_file)
    
    # Load words
    if args.test_mode:
        logger.info("Using test mode with sample data")
        words_data = [
            {'lemma': 'hundo', 'paradigm': 'o__n'},
            {'lemma': 'kato', 'paradigm': 'o__n'},
            {'lemma': 'bona', 'paradigm': 'a__adj'},
            {'lemma': 'irar', 'paradigm': 'ar__vblex'},
        ]
    else:
        if not args.extractor_file:
            logger.error("Extractor file required when not in test mode")
            sys.exit(1)
        logger.info(f"Loading words from: {args.extractor_file}")
        words_data = load_extractor_words(args.extractor_file)
    
    # Merge and write
    logger.info(f"Writing merged dictionary to: {args.output_file}")
    merge_dictionary(rules_root, words_data, args.output_file)
    
    # Validate if requested
    if args.validate:
        validate_dictionary(args.output_file)

if __name__ == '__main__':
    main()
