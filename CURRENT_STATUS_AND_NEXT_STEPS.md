# Current Status & Critical Decision Needed

**Date:** October 16, 2025

---

## üîç **Critical Finding**

After extensive filtering and analysis, we've discovered that **Wikipedia article titles are predominantly proper nouns** (people, places, organizations), not common vocabulary.

### What We Have

| Category | Count | What It Contains |
|----------|-------|------------------|
| **Total extracted** | 5,031 | All entries with morphology |
| Proper nouns (tagged) | 1,795 | Detected geographic/borrowed names |
| Regular nouns | 2,539 | Mix of vocabulary + undetected places |
| Adjectives | 404 | Mix of descriptive + place adjectives |
| Verbs | 188 | Mix of actions + places ending in verb forms |
| Adverbs | 105 | Mix of manner + places ending in -e |

---

## ‚ö†Ô∏è **The Core Problem**

**Ido gives grammatical endings to ALL nouns, including place names:**

```
Place Name Examples:
  Aarhus (city) ‚Üí Aarh-us (looks like verb conditional)
  Abhazia (region) ‚Üí Abhazi-a (looks like adjective)
  Acre (state) ‚Üí Acr-e (looks like adverb)
  Aconcagua (mountain) ‚Üí Aconcagu-a (looks like adjective)
```

**These ARE valid Ido words**, but they're **proper nouns**, not common vocabulary.

---

## üìä **Estimated Actual Breakdown**

Based on manual review of test samples:

| Type | Estimated Count | Percentage |
|------|----------------|------------|
| **True common vocabulary** | ~1,000-1,500 | 20-30% |
| **Geographic proper nouns** | ~2,500-3,000 | 50-60% |
| **People/org names** | ~500-1,000 | 10-20% |
| **Technical/scientific terms** | ~500-800 | 10-15% |

---

## üí° **Three Possible Approaches**

### **Approach A: Accept Mixed Content** ‚≠ê RECOMMENDED
**What:** Add all 5,031 entries with proper POS tagging
**Pros:**
- Proper nouns are useful for translation!
- "me iras a Acapulco" ‚Üí "mi iras al Akapulko" (needs dict entry)
- Complete coverage
**Cons:**
- Dictionary will be 60% proper nouns
- Not "pure vocabulary"
**Result:** ~13,000 entry dictionary (comprehensive)

### **Approach B: Manual Review & Selection**
**What:** Manually review all 5,031 and select only true vocabulary
**Pros:**
- Highest quality vocabulary
- Pure dictionary of common words
**Cons:**
- Time intensive (10-20 hours of review)
- Miss useful proper nouns
**Result:** ~1,500-2,000 pure vocabulary additions

### **Approach C: Create Two Separate Dictionaries**
**What:** 
- Dictionary A: Common vocabulary only (~1,500 words)
- Dictionary B: Proper nouns (~3,500 words)
**Pros:**
- Clear separation
- Can load both or just vocabulary
**Cons:**
- More complex setup
- Need to maintain two files
**Result:** Two specialized dictionaries

---

## üéØ **My Strong Recommendation: Approach A**

**Why:**
1. Proper nouns ARE needed for good translation
2. User will translate sentences about cities, countries, people
3. Having "Acapulco"‚Üí"Akapulko" in dict is useful
4. Can filter dictionary later if needed
5. Saves 10-20 hours of manual review

**Implementation:**
1. Use `wikipedia_vocabulary_merge_ready.json` (5,031 entries)
2. All entries properly tagged (np for proper nouns, n/vblex/adj/adv for vocabulary)
3. Test with 200-word sample
4. If good, merge all 5,031 entries
5. Result: Comprehensive dictionary with both vocabulary AND proper nouns

---

## üìã **What Needs to Happen Next**

### **If You Choose Approach A (Recommended):**

1. ‚úÖ Generate new test sample from merge-ready vocab
2. ‚úÖ Merge test sample with current dictionary  
3. ‚úÖ Generate test .dix files
4. ‚úÖ Validate XML
5. ‚ö†Ô∏è **MANUAL STEP:** Test a few translations to verify quality
6. ‚úÖ If good, merge all 5,031 entries
7. ‚úÖ Regenerate full dictionaries
8. ‚úÖ Rebuild and test translation system

**Time:** ~1 hour (mostly automated)

### **If You Choose Approach B (Manual Review):**

1. Export vocabulary to spreadsheet
2. Manually mark each entry: keep/skip
3. Filter based on markings
4. Proceed with merge of selected entries only

**Time:** 10-20 hours of review

### **If You Choose Approach C (Two Dictionaries):**

1. Split into vocabulary.json and proper_nouns.json
2. Create separate merge scripts
3. Generate two sets of .dix files
4. Configure Apertium to use both

**Time:** ~2-3 hours implementation

---

## ‚ùì **Decision Point**

**Which approach do you want to take?**

- **A**: Add all 5,031 (vocabulary + proper nouns mixed) - FAST, COMPREHENSIVE
- **B**: Manual review to get ~1,500 pure vocabulary - SLOW, PURE
- **C**: Create two dictionaries - MEDIUM, COMPLEX

**Or should I:**
- Show you more samples to help decide?
- Create a different filtering strategy?
- Something else?

---

## üìä **Current Files Status**

‚úÖ **Ready for merge:**
- `wikipedia_vocabulary_merge_ready.json` (5,031 entries, properly tagged)

‚úÖ **Already created:**
- Test dictionaries (validated XML)
- All filtering scripts
- Morphology analysis complete
- Proper POS tagging complete

‚è≥ **Waiting for:**
- Your decision on approach
- Final merge approval

---

**What would you like to do?**

